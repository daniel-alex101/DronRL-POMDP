{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjzo77F4ItHSdHi9YB6R5j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-alex101/DronRL-POMDP/blob/main/GTrXL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer GTrXL en resolución de observabilidad parcial en Aprendizaje por Refuerzo\n"
      ],
      "metadata": {
        "id": "wXIHq1eyKDAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer GTrXL basado en la implementación de GitHub de Alan:\n",
        "https://github.com/alantess/gtrxl-torch"
      ],
      "metadata": {
        "id": "A5H9WBXiK57U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch as T\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch import Tensor\n",
        "import math\n",
        "import sys\n",
        "from stable_baselines3.common.distributions import DiagGaussianDistribution\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "# Positional encoding del transformer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout) # Capa de dropout\n",
        "        pe = T.zeros(max_len, d_model)\n",
        "        position = T.arange(0, max_len, dtype=T.float).unsqueeze(1) # Generar índices para las posiciones\n",
        "        div_term = T.exp(\n",
        "            T.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Divisor de la fórmula del PE\n",
        "        pe[:, 0::2] = T.sin(position * div_term) # seno para los pares\n",
        "        pe[:, 1::2] = T.cos(position * div_term) # coseno para los impares\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :] # Sumar PE en el forward\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Codificador del transformer\n",
        "class TEL(TransformerEncoderLayer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 n_layers=1,\n",
        "                 dim_feedforward=256,\n",
        "                 activation=\"relu\",\n",
        "                 dropout=0,\n",
        "                 layer_norm_eps=1e-5,\n",
        "                 batch_first=False):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation,\n",
        "                         layer_norm_eps, batch_first)\n",
        "        # 2 GRUs: una al comienzo y otra al final\n",
        "        self.gru_1 = nn.GRU(d_model,\n",
        "                            d_model,\n",
        "                            num_layers=n_layers,\n",
        "                            batch_first=True)\n",
        "        self.gru_2 = nn.GRU(input_size=d_model,\n",
        "                            hidden_size=d_model,\n",
        "                            num_layers=n_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                is_causal: Optional[bool] = None ) -> Tensor:\n",
        "        h = (src).sum(dim=1).unsqueeze(dim=0) # Calcular estado inicial h de las GRU\n",
        "        src = self.norm1(src) # Aplicar primer layer norm\n",
        "\n",
        "        # Atención\n",
        "        out = self.self_attn(src,\n",
        "                             src,\n",
        "                             src,\n",
        "                             attn_mask=src_mask,\n",
        "                             key_padding_mask=src_key_padding_mask)[0]\n",
        "\n",
        "        out, h = self.gru_1(out, h) # Aplicar primera GRU con su estado h\n",
        "        out = self.norm2(out) # Segunda leyer norm del transformer\n",
        "\n",
        "        # Aplicando capas lineales del transformer sobre la normalización\n",
        "        out = self.activation(self.linear1(out))\n",
        "        out = self.activation(self.linear2(out))\n",
        "        out, h = self.gru_2(out, h) # Obtener salida final con la segunda GRU\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# GTrXL final\n",
        "class GTrXL(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nheads,\n",
        "                 transformer_layers,\n",
        "                 hidden_dims=256,\n",
        "                 n_layers=1,\n",
        "                 layer_norm_eps=1e-5,\n",
        "                 batch_first=False,\n",
        "                 chkpt_dir=\"models\",\n",
        "                 activation='relu',\n",
        "                 network_name='network.pt'):\n",
        "        super(GTrXL, self).__init__()\n",
        "\n",
        "        # Positional encoding\n",
        "        self.embed = PositionalEncoding(d_model)\n",
        "        encoded = TEL(d_model,\n",
        "                      nheads,\n",
        "                      n_layers,\n",
        "                      dim_feedforward=hidden_dims,\n",
        "                      activation=activation,\n",
        "                      layer_norm_eps=layer_norm_eps,\n",
        "                      batch_first=batch_first)\n",
        "        self.transfomer = TransformerEncoder(encoded, transformer_layers)\n",
        "        self.file = os.path.join(chkpt_dir, network_name)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.transfomer(x)\n",
        "        return x\n",
        "\n",
        "    def save(self):\n",
        "        T.save(self.state_dict(), self.file)\n",
        "\n",
        "    def load(self):\n",
        "        self.load_state_dict(T.load(self.file))"
      ],
      "metadata": {
        "id": "7up7dVo_Lj5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entorno de simulación de redes móviles con drones (esto no es parte de la ayudantía sino del proyecto de trabajo de título)\n"
      ],
      "metadata": {
        "id": "9ajpIEbiLrXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box, Dict, Discrete\n",
        "import numpy as np\n",
        "from scipy import constants\n",
        "from math import pi\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "from stable_baselines3 import PPO\n",
        "import sys\n",
        "\n",
        "\n",
        "#REWARDS\n",
        "class Person:\n",
        "  def __init__(self, position):\n",
        "    self.position = position\n",
        "  # Mueve a la persona en una direccion asegurandose que no se salga del grid\n",
        "  def take_step(self, n, m, direction):\n",
        "    return np.clip(self.position + direction, [0, 0], [n - 1, m - 1])\n",
        "\n",
        "def RewardMetrics(distances):\n",
        "  #constantes\n",
        "  fc=28e9\n",
        "  lambda_s=constants.speed_of_light/fc\n",
        "  Pt=10e-3\n",
        "  Pt_dB=10*np.log10(Pt) #-20\n",
        "  #variacion de ruido\n",
        "  N0_dbm=-170\n",
        "  N0=10**((N0_dbm-30)/10)\n",
        "  B=1e6 #Hz\n",
        "  sigma2_u=N0*B\n",
        "  sigma2_u_dB=10*np.log10(sigma2_u)\n",
        "  gamma=1\n",
        "  d0=1\n",
        "  metrics=[]\n",
        "  k_dB=10*np.log10(((lambda_s/4*pi*d0))**2)#duda\n",
        "  for distance in distances.values():\n",
        "      line= distance\n",
        "      PL_dB=k_dB-10*gamma*np.log10((line / d0)**2)\n",
        "      Pl=10**(PL_dB/10)\n",
        "      R=B*np.log2(1+(Pt*Pl/sigma2_u))\n",
        "      Cost=R*1e-6\n",
        "      metrics.append(np.mean(Cost))\n",
        "  return metrics\n",
        "\n",
        "def get_distances(agent_position, height, people):\n",
        "  agent_position_3d = np.append(agent_position, height)\n",
        "  distances = {}\n",
        "  for i, person in enumerate(people):\n",
        "    people_position_3d = np.append(person.position, 0)\n",
        "    distance = np.linalg.norm(agent_position_3d - people_position_3d)\n",
        "    distances[i] = distance\n",
        "  return distances\n",
        "\n",
        "def Total_signal_grid(agent_position, height, people):\n",
        "  distances_grid = get_distances(agent_position, height, people)\n",
        "  metrics_grid = RewardMetrics(distances_grid)\n",
        "  return sum(metrics_grid)\n",
        "\n",
        "def StdDev_signal_grid(agent_position, height, people):\n",
        "  distances_grid = get_distances(agent_position, height, people)\n",
        "  metrics_grid = RewardMetrics(distances_grid)\n",
        "  return np.std(metrics_grid)\n",
        "# Recompensa en base a la señal dependiendo de la distancia\n",
        "def Reward1(distances):\n",
        "  fc=28e9\n",
        "  lambda_s=constants.speed_of_light/fc\n",
        "  Pt=10e-3\n",
        "  Pt_dB=10*np.log10(Pt) #-20\n",
        "  #variacion de ruido\n",
        "  N0_dbm=-170\n",
        "  N0=10**((N0_dbm-30)/10)\n",
        "  B=1e6 #Hz\n",
        "  sigma2_u=N0*B\n",
        "  sigma2_u_dB=10*np.log10(sigma2_u)\n",
        "  gamma=1\n",
        "  d0=1\n",
        "  metrics=[]\n",
        "  k_dB=10*np.log10(((lambda_s/4*pi*d0))**2)#duda\n",
        "  for distance in distances.values():\n",
        "      line= distance\n",
        "      PL_dB=k_dB-10*gamma*np.log10((line / d0)**2)\n",
        "      Pl=10**(PL_dB/10)\n",
        "      R=B*np.log2(1+(Pt*Pl/sigma2_u))\n",
        "      Cost=R*1e-6\n",
        "      metrics.append(Cost)\n",
        "  return np.mean(metrics)\n",
        "\n",
        "#Recompensa con umbral\n",
        "def Reward2(threshold, distances):\n",
        "  costs = RewardMetrics(distances)\n",
        "\n",
        "  # Cantidad de personas\n",
        "  n_people = len(distances)\n",
        "  n_people_above_treshold = 0\n",
        "  for cost in costs:\n",
        "    if cost > threshold:\n",
        "      n_people_above_treshold += 1\n",
        "  perc_people_above_treshold = n_people_above_treshold / n_people\n",
        "\n",
        "  # Con porcentaje de personas\n",
        "  if perc_people_above_treshold == 1:\n",
        "    return 5, np.mean(costs), perc_people_above_treshold\n",
        "  elif perc_people_above_treshold > 0.8:\n",
        "    return 3, np.mean(costs), perc_people_above_treshold\n",
        "  elif perc_people_above_treshold > 0.5:\n",
        "    return 1, np.mean(costs), perc_people_above_treshold\n",
        "  else:\n",
        "    return 0, np.mean(costs), perc_people_above_treshold\n",
        "\n",
        "# Recompensa con diferencia de señal\n",
        "def Reward3(old_distances, new_distances):\n",
        "  old_metrics = RewardMetrics(old_distances).copy()\n",
        "  old_mean = np.mean(old_metrics)\n",
        "  new_metrics = RewardMetrics(new_distances).copy()\n",
        "  new_mean = np.mean(new_metrics)\n",
        "  delta = new_mean - old_mean\n",
        "  return delta, new_mean\n",
        "\n",
        "def Calc_threshold(n,m,height):\n",
        "  distances_threshold = {}\n",
        "  agent_position_min = np.array([ 0, 0, height])\n",
        "  person_position_min = np.array([ 0, 0, 0])\n",
        "  agent_position_max = np.array([n-1, m-1, height])\n",
        "\n",
        "  distance_min = np.linalg.norm(agent_position_min - person_position_min)\n",
        "  distances_threshold[0] = distance_min\n",
        "  distance_max = np.linalg.norm(agent_position_max - person_position_min)\n",
        "  distances_threshold[1] = distance_max\n",
        "\n",
        "  metrics_threshold = RewardMetrics(distances_threshold).copy()\n",
        "  mean_threshold= np.mean(metrics_threshold)\n",
        "  return mean_threshold\n",
        "\n",
        "def get_distances(agent_position,height,people):\n",
        "  agent_position_3d = np.append(agent_position, height)\n",
        "  distances = {}\n",
        "  for i, person in enumerate(people):\n",
        "    people_position_3d = np.append(person.position, 0)\n",
        "    distance = np.linalg.norm(agent_position_3d - people_position_3d)\n",
        "    distances[i] = distance\n",
        "  return distances\n",
        "\n",
        "# Cambiar a cantidad de señal\n",
        "def IdealCaseReward1(n, m,action_to_direction,ideal_agent_pos,height,people):\n",
        "  maxSignalReward = 0\n",
        "  best_action = 0\n",
        "  for i in range(9):\n",
        "    direction = action_to_direction[i]\n",
        "    new_pos = ideal_agent_pos + direction\n",
        "    new_pos = np.clip(new_pos, [0, 0], [n - 1, m - 1])\n",
        "    reward_signal = Reward1(get_distances(new_pos,height,people))\n",
        "\n",
        "    if reward_signal > maxSignalReward:\n",
        "      maxSignalReward = reward_signal\n",
        "      best_action = i\n",
        "\n",
        "  direction = action_to_direction[best_action]\n",
        "  ideal_pos = ideal_agent_pos + direction\n",
        "  ideal_pos = np.clip(ideal_pos, [0, 0], [n - 1, m - 1])\n",
        "  return ideal_pos, maxSignalReward\n",
        "\n",
        "\n",
        "def IdealCaseReward2(n, m, threshold, action_to_direction,ideal_agent_pos,height,people):\n",
        "  maxNumberPeople = 0\n",
        "  maxMeanSignal = 0\n",
        "  best_action = 0\n",
        "  max_reward = 0\n",
        "  for i in range(9):\n",
        "    direction = action_to_direction[i]\n",
        "    new_pos = ideal_agent_pos + direction\n",
        "    new_pos = np.clip(new_pos, [0, 0], [n - 1, m - 1])\n",
        "    reward,mean_signal,people_above_th = Reward2(threshold, get_distances(new_pos,height,people))\n",
        "\n",
        "    # Incluir el promedio de señal para decidir entre cantidad de personas por sobre el umbral\n",
        "    if people_above_th >= maxNumberPeople:\n",
        "      if people_above_th == maxNumberPeople:\n",
        "        if mean_signal > maxMeanSignal:\n",
        "          maxMeanSignal = mean_signal\n",
        "          maxNumberPeople = people_above_th\n",
        "          best_action = i\n",
        "          max_reward = reward\n",
        "      else:\n",
        "        maxMeanSignal = mean_signal\n",
        "        maxNumberPeople = people_above_th\n",
        "        best_action = i\n",
        "        max_reward = reward\n",
        "  direction = action_to_direction[best_action]\n",
        "  ideal_pos = ideal_agent_pos + direction\n",
        "  ideal_pos = np.clip(ideal_pos, [0, 0], [n - 1, m - 1])\n",
        "  return ideal_pos, max_reward, maxMeanSignal\n",
        "\n",
        "def IdealCaseReward3(n, m, action_to_direction,ideal_agent_pos,height,people, old_distances):\n",
        "  maxSignalReward = 0\n",
        "  best_action = 0\n",
        "  new_mean_signal = 0\n",
        "  for i in range(9):\n",
        "    direction = action_to_direction[i]\n",
        "    new_pos = ideal_agent_pos + direction\n",
        "    new_pos = np.clip(new_pos, [0, 0], [n - 1, m - 1])\n",
        "    reward, mean_signal = Reward3(old_distances, get_distances(new_pos,height,people))\n",
        "\n",
        "    if reward > maxSignalReward:\n",
        "      maxSignalReward = reward\n",
        "      best_action = i\n",
        "      new_mean_signal = mean_signal\n",
        "\n",
        "  direction = action_to_direction[best_action]\n",
        "  ideal_pos = ideal_agent_pos + direction\n",
        "  ideal_pos = np.clip(ideal_pos, [0, 0], [n - 1, m - 1])\n",
        "  return ideal_pos, maxSignalReward, new_mean_signal\n",
        "\n",
        "\n",
        "def getActionProbs(seedo):\n",
        "  primo1 = 2\n",
        "  primo2 = 3\n",
        "  primo3 = 5\n",
        "  primo4 = 7\n",
        "  primo5 = 11\n",
        "  primo6 = 13\n",
        "  primo7 = 17\n",
        "  primo8 = 19\n",
        "  primo9 = 23\n",
        "  primo10 = 29\n",
        "  prob = np.random.permutation(np.array([ 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.8]))\n",
        "\n",
        "  if seedo % primo1 == 0:\n",
        "    #print(\"Acción: Stay in place\")\n",
        "    prob = np.array([0.8, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]) # Acción: Stay in place\n",
        "  elif seedo % primo2 == 0:\n",
        "    #print(\"Acción: ↖\")\n",
        "    prob = np.array([ 0.025, 0.8, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]) # Acción: ↖\n",
        "  elif seedo % primo3 == 0:\n",
        "    #print(\"Acción: ↑\")\n",
        "    prob = np.array([ 0.025, 0.025, 0.8, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]) # Acción: ↑\n",
        "  elif seedo % primo4 == 0:\n",
        "    #print(\"Acción: ↗\")\n",
        "    prob = np.array([ 0.025, 0.025, 0.025, 0.8, 0.025, 0.025, 0.025, 0.025, 0.025]) # Acción: ↗\n",
        "  elif seedo % primo5 == 0:\n",
        "    #print(\"Acción: →\")\n",
        "    prob = np.array([ 0.025, 0.025, 0.025, 0.025, 0.8, 0.025, 0.025, 0.025, 0.025]) # Acción: →\n",
        "  elif seedo % primo6 == 0:\n",
        "    #print(\"Acción: ↘\")\n",
        "    prob = np.array([ 0.025, 0.025, 0.025, 0.025, 0.025, 0.8, 0.025, 0.025, 0.025]) # Acción: ↘\n",
        "  elif seedo % primo7 == 0:\n",
        "    #print(\"Acción: ↓\")\n",
        "    prob = np.array([ 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.8, 0.025, 0.025]) # Acción: ↓\n",
        "  elif seedo % primo8 == 0:\n",
        "    #print(\"Acción: ↙\")\n",
        "    prob = np.array([ 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.8, 0.025]) # Acción: ↙\n",
        "  elif seedo % primo9 == 0:\n",
        "    #print(\"Acción: ←\")\n",
        "    prob = np.array([ 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.8]) # Acción: ←\n",
        "  return prob\n",
        "\n",
        "#------------------------------ -------------------CUSTOM ENV -------------------------------------------------------------\n",
        "class CustomEnv3(gym.Env):\n",
        "  def __init__(self, n, m, max_steps, height, people_quantity, seed=None):\n",
        "    super(CustomEnv3, self).__init__()\n",
        "    self.n = n\n",
        "    self.m = m\n",
        "    self.max_steps = max_steps\n",
        "    self.height = height\n",
        "    self.people_quantity = people_quantity\n",
        "    self.seed = seed\n",
        "    self.threshold = Calc_threshold(self.n,self.m,self.height)\n",
        "\n",
        "    self.observation_space = Dict(\n",
        "    {\n",
        "      \"grid\": Box(low=0, high=self.people_quantity, shape=(n*m*3,), dtype=np.int32),  # Grid\n",
        "      \"x_agent\": Discrete(n),  # Agent's x position\n",
        "      \"y_agent\": Discrete(m),  # Agent's y position\n",
        "    }, seed)\n",
        "\n",
        "    # Define action space (9 possible actions)\n",
        "    self.action_space = Discrete(9)\n",
        "\n",
        "    # Mapping of actions to directions\n",
        "    self._action_to_direction = {\n",
        "        0: np.array([0, 0]),    # Stay in place\n",
        "        1: np.array([-1, -1]),  # ↖\n",
        "        2: np.array([0, -1]),   # ↑\n",
        "        3: np.array([1, -1]),   # ↗\n",
        "        4: np.array([1, 0]),    # →\n",
        "        5: np.array([1, 1]),    # ↘\n",
        "        6: np.array([0, 1]),    # ↓\n",
        "        7: np.array([-1, 1]),   # ↙\n",
        "        8: np.array([-1, 0]),   # ←\n",
        "    }\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self, seed=None):\n",
        "    self.static_agent_pos = np.array([self.n // 2, self.m // 2])\n",
        "    self.rewards_static_agent = []\n",
        "    self.ideal_agent_pos = np.array([self.n // 2, self.m // 2])\n",
        "    self.rewards_ideal_agent = []\n",
        "\n",
        "    self.current_step = 0\n",
        "\n",
        "    # Probabilidades de personas\n",
        "    self.probabilities = getActionProbs(self.seed)  ###CAMBIO\n",
        "\n",
        "    # Se devuelve al agente al centro del grid\n",
        "    self.agent_position = np.array([self.n // 2, self.m // 2])\n",
        "\n",
        "    # Se actualiza el historial de estados de la matriz de personas\n",
        "    self.grid = np.zeros((self.n, self.m), dtype=np.int32)\n",
        "    self.grid_1step_prev = np.zeros((self.n, self.m), dtype=np.int32)\n",
        "    self.grid_2step_prev = np.zeros((self.n, self.m), dtype=np.int32)\n",
        "\n",
        "    # Se colocan aleatoriamente las personas en la matriz\n",
        "    people = []\n",
        "    local_rng = np.random.default_rng(self.seed)\n",
        "    for _ in range(self.people_quantity):\n",
        "        if self.seed % 29 == 0:\n",
        "            x, y = np.random.randint(self.n), np.random.randint(self.m)\n",
        "        else:\n",
        "            x = local_rng.integers(self.n)\n",
        "            y = local_rng.integers(self.m)\n",
        "        person = Person([x, y])\n",
        "        people.append(person)\n",
        "        self.grid[x, y] += 1\n",
        "\n",
        "    self.people = people.copy()\n",
        "    self.state = self.observation_space.sample()\n",
        "    info = self._get_info()\n",
        "    obs = self._get_obs()\n",
        "    return obs, info\n",
        "\n",
        "\n",
        "  def _get_info(self):\n",
        "    info = {\n",
        "        'current_step': self.current_step,\n",
        "        'agent_position': self.agent_position,\n",
        "        'ideal_agent_position': self.ideal_agent_pos,\n",
        "        'static_agent_position': self.static_agent_pos,\n",
        "        'people_positions': self.people,\n",
        "        'Total_signal': Total_signal_grid(self.agent_position, self.height, self.people),\n",
        "        'Total_signal_static': Total_signal_grid(self.static_agent_pos, self.height, self.people),\n",
        "        'Total_signal_ideal': Total_signal_grid(self.ideal_agent_pos, self.height, self.people),\n",
        "        'Std_dev_signal': StdDev_signal_grid(self.agent_position, self.height, self.people),\n",
        "        'Std_dev_signal_static': StdDev_signal_grid(self.static_agent_pos, self.height, self.people),\n",
        "        'Std_dev_signal_ideal': StdDev_signal_grid(self.ideal_agent_pos, self.height, self.people)\n",
        "    }\n",
        "    return info\n",
        "\n",
        "  def agent_move(self, action):\n",
        "    direction = self._action_to_direction[action]\n",
        "    new_position = self.agent_position + direction\n",
        "    return np.clip(new_position, [0, 0], [self.n - 1, self.m - 1])\n",
        "\n",
        "  def step(self, action):\n",
        "    # Se mueve el agente y se actualiza su posición\n",
        "    old_distances = get_distances(self.agent_position, self.height, self.people)\n",
        "    old_distances_static = get_distances(self.static_agent_pos, self.height, self.people)\n",
        "    old_distances_ideal = get_distances(self.ideal_agent_pos,self.height,self.people)\n",
        "    self.agent_position = self.agent_move(action).copy()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Se actualiza la posición de las personas\n",
        "    self.grid_2step_prev = np.copy(self.grid_1step_prev)\n",
        "    self.grid_1step_prev = np.copy(self.grid)\n",
        "    posible_actions = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
        "\n",
        "    # Todas las personas realizan un paso considerando las probabilidades\n",
        "    people_new_positions = []\n",
        "    for person in self.people:\n",
        "      action_direction = np.random.choice(posible_actions, p=self.probabilities)\n",
        "      direction = self._action_to_direction[action_direction]\n",
        "      person_new_position = person.take_step(self.n, self.m, direction).copy()\n",
        "      if action_direction != 0:\n",
        "          self.grid[person.position[0], person.position[1]] -= 1\n",
        "          self.grid[person_new_position[0], person_new_position[1]] += 1\n",
        "      person_updated = Person(person_new_position)\n",
        "      people_new_positions.append(person_updated)\n",
        "\n",
        "    self.people = people_new_positions.copy()\n",
        "    new_distances = get_distances(self.agent_position, self.height, self.people)\n",
        "\n",
        "    #Reward 3\n",
        "    reward, new_mean = Reward3(old_distances, new_distances)\n",
        "\n",
        "    # Recompensas de otros drones\n",
        "    new_distances_static = get_distances(self.static_agent_pos, self.height, self.people)\n",
        "    reward_static_agent, static_new_mean = Reward3(old_distances_static, new_distances_static)\n",
        "    self.ideal_agent_pos, reward_ideal_agent, ideal_new_mean = IdealCaseReward3(self.n, self.m, self._action_to_direction, self.ideal_agent_pos, self.height, self.people, old_distances_ideal)\n",
        "\n",
        "    # Check if the episode is done\n",
        "    info = self._get_info()\n",
        "    observation = self._get_obs()\n",
        "    self.current_step += 1\n",
        "    done = self.current_step >= self.max_steps\n",
        "\n",
        "    truncated = False\n",
        "    return observation, reward, done, truncated, info\n",
        "\n",
        "\n",
        "\n",
        "class RewardCallback(BaseCallback):\n",
        "    def __init__(self):\n",
        "        super(RewardCallback, self).__init__()\n",
        "        self.episode_rewards = []\n",
        "        self.episode_infos = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if len(self.locals['infos']) > 0:\n",
        "            for info in self.locals['infos']:\n",
        "                if 'episode' in info.keys():\n",
        "                    self.episode_rewards.append(info['episode']['r'])\n",
        "                    self.episode_infos.append(info)\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "    def _on_training_end(self):\n",
        "\n",
        "        np.savetxt(\"./models/rewards3/transformer1_reward\"+\"_\"+task_id+\".txt\", self.episode_rewards, delimiter=\",\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yuGdaz9lL3oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modificación de observabilidad parcial 1: pasando solo el estado actual y no los dos anteriores"
      ],
      "metadata": {
        "id": "00s8-DIBMNCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PartialObservationEnv(CustomEnv3):\n",
        "    def __init__(self, n, m, max_steps, height, people_quantity, seed=29):\n",
        "\n",
        "        super().__init__(n, m, max_steps, height, people_quantity, seed)\n",
        "\n",
        "\n",
        "        self.observation_space = gym.spaces.Dict({\n",
        "\n",
        "            \"grid\": Box(low=0, high=self.people_quantity, shape=(n,m), dtype=np.int32),\n",
        "            \"x_agent\": gym.spaces.Discrete(n),\n",
        "            \"y_agent\": gym.spaces.Discrete(m)\n",
        "        })\n",
        "\n",
        "    def _get_obs(self):\n",
        "\n",
        "        x, y = self.agent_position\n",
        "\n",
        "        return {\n",
        "            \"grid\": self.grid, # Solo la observación actual sin concatenar las dos anteriores\n",
        "            \"x_agent\": x,\n",
        "            \"y_agent\": y\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Y4i9Q2pZMMDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modificación de observabilidad parcial 2: pasando eliminando aleatoriamente una persona de la visión local"
      ],
      "metadata": {
        "id": "AGY1e3QZMQRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PartialObservationEnv(CustomEnv3):\n",
        "    def __init__(self, n, m, max_steps, height, people_quantity, seed=29):\n",
        "\n",
        "        super().__init__(n, m, max_steps, height, people_quantity, seed)\n",
        "\n",
        "\n",
        "        self.observation_space = gym.spaces.Dict({\n",
        "\n",
        "            \"partial_grid\": Box(low=0, high=self.people_quantity, shape=(n*m*3,), dtype=np.int32),\n",
        "            \"x_agent\": gym.spaces.Discrete(n),\n",
        "            \"y_agent\": gym.spaces.Discrete(m)\n",
        "        })\n",
        "\n",
        "    def _get_obs(self):\n",
        "\n",
        "        x, y = self.agent_position\n",
        "\n",
        "\n",
        "        # Grid parcial de donde se quitan las personas\n",
        "        partial_grid = np.zeros((self.n,self.m))\n",
        "\n",
        "        person_mask = self.grid == 1\n",
        "\n",
        "        random_values = np.random.randint(0,4, size=person_mask.shape) # Seleccionando posiciones aleatorias a quitar\n",
        "\n",
        "        partial_grid[person_mask] = np.where(random_values[person_mask] == 0, 0, self.grid[person_mask])\n",
        "\n",
        "        partial_grid = np.concatenate((self.grid_2step_prev.flatten(), self.grid_1step_prev.flatten(), partial_grid.flatten()))\n",
        "\n",
        "        return {\n",
        "            \"partial_grid\": partial_grid, # pasando grid parcial como estado\n",
        "            \"x_agent\": x,\n",
        "            \"y_agent\": y\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Av9q7D-oMJGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integración de entorno de Gym con transformer en Stable-Baselines 3\n"
      ],
      "metadata": {
        "id": "TqpoFw0PMmcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor, MlpExtractor"
      ],
      "metadata": {
        "id": "nxJ5oA59MzP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extractor 1: El grid se pasa por una CNN antes de pasarse por el transformer\n"
      ],
      "metadata": {
        "id": "gTRY9kGWMz7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GTrXLFeatureExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, d_model, nheads, transformer_layers, conv_channels=32, kernel_size=3, stride=1, padding=1):\n",
        "\n",
        "\n",
        "        super().__init__(observation_space, features_dim=d_model)\n",
        "\n",
        "        # Obtener el tamaño del grid y las posiciones del agente\n",
        "        grid_shape = observation_space[\"grid\"].shape\n",
        "        agent_pos_size = observation_space[\"x_agent\"].n + observation_space[\"y_agent\"].n  # Tamaño de las posiciones del agente\n",
        "\n",
        "        # Definir las capas de la CNN\n",
        "        self.conv1 = nn.Conv2d(1, conv_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv2 = nn.Conv2d(conv_channels, conv_channels * 2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "        # Capa de pooling para reducir el tamaño del grid\n",
        "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "\n",
        "        # Capa de embedding para las posiciones del agente\n",
        "        self.agent_pos_embedding = nn.Linear(agent_pos_size, d_model)\n",
        "\n",
        "        # Capa de embedding para la entrada concatenada (CNN + posiciones del agente)\n",
        "        self.input_embedding = nn.Linear(conv_channels * 2 * 4 * 4 + d_model, d_model)\n",
        "\n",
        "        # Definiendo el modelo GTrXL\n",
        "        self.gtrxl = GTrXL(\n",
        "            d_model=d_model,\n",
        "            nheads=nheads,\n",
        "            transformer_layers=transformer_layers,\n",
        "            hidden_dims=d_model,\n",
        "            n_layers=1\n",
        "        )\n",
        "\n",
        "    def forward(self, observations):\n",
        "        # Obtener el tamaño del batch\n",
        "        batch_size = observations[\"grid\"].shape[0]\n",
        "\n",
        "\n",
        "        # Asegurarse de que el grid tenga la forma (batch_size, 1, height, width)\n",
        "        grid = observations[\"grid\"].view(batch_size, 1, *observations[\"grid\"].shape[1:])\n",
        "\n",
        "        # Procesar el grid con la CNN\n",
        "        x = self.conv1(grid)\n",
        "        x = T.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = T.relu(x)\n",
        "        x = self.pool(x)  # Aplicar el pooling\n",
        "\n",
        "        # Aplanar el tensor después de la CNN\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # Obtener las posiciones del agente y procesarlas con una capa lineal\n",
        "        x_agent = T.tensor(observations[\"x_agent\"], dtype=T.float32).view(batch_size, -1)\n",
        "        y_agent = T.tensor(observations[\"y_agent\"], dtype=T.float32).view(batch_size, -1)\n",
        "\n",
        "        # Concatenar las posiciones del agente y el grid procesado\n",
        "        agent_pos = T.cat([x_agent, y_agent], dim=1)\n",
        "        agent_pos = self.agent_pos_embedding(agent_pos)\n",
        "\n",
        "        # Concatenar las características del grid y las posiciones del agente\n",
        "        x = T.cat([x, agent_pos], dim=1)\n",
        "\n",
        "        # Embedding de las entradas concatenadas\n",
        "        x = self.input_embedding(x)\n",
        "\n",
        "        # Añadir dimensión de secuencia para el Transformer\n",
        "        x = x.unsqueeze(1)  # (batch_size, 1, d_model)\n",
        "\n",
        "        # Pasar por el modelo GTrXL\n",
        "        features = self.gtrxl(x)\n",
        "\n",
        "        # Tomar la última salida de la dimensión de secuencia\n",
        "        features = features[:, -1, :]  # Usar el último dato de la secuencia\n",
        "\n",
        "        return features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rb_H2j_TM9Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extractor 2: Se pasa directamente la observación al transformer"
      ],
      "metadata": {
        "id": "kBrB6LcKNTur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GTrXLFeatureExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, d_model=256, nheads=4, transformer_layers=3):\n",
        "        super().__init__(observation_space, features_dim=d_model)\n",
        "\n",
        "\n",
        "        # Calcular tamaño de la grilla\n",
        "        local_grid_size = int(np.prod(observation_space[\"partial_grid\"].shape))\n",
        "        agent_pos_size = observation_space[\"x_agent\"].n + observation_space[\"y_agent\"].n\n",
        "        n_input = local_grid_size + agent_pos_size\n",
        "\n",
        "        self.input_embedding = nn.Linear(n_input, d_model)\n",
        "        self.gtrxl = GTrXL(\n",
        "            d_model=d_model,\n",
        "            nheads=nheads,\n",
        "            transformer_layers=transformer_layers,\n",
        "            hidden_dims=d_model,\n",
        "            n_layers=1\n",
        "        )\n",
        "\n",
        "    def forward(self, observations):\n",
        "\n",
        "\n",
        "            batch_size = observations[\"partial_grid\"].shape[0]\n",
        "            local_grid_flat = observations[\"partial_grid\"].clone().detach().float().view(batch_size, -1)\n",
        "\n",
        "            # Obtener posiciones del agente\n",
        "            x_agent = observations[\"x_agent\"].clone().detach().float().view(batch_size, -1)\n",
        "            y_agent = observations[\"y_agent\"].clone().detach().float().view(batch_size, -1)\n",
        "            # Concatenar los elementos del estado (posiciones y grilla)\n",
        "            x = T.cat([\n",
        "                local_grid_flat,\n",
        "                x_agent,\n",
        "                y_agent\n",
        "            ], dim=1)\n",
        "\n",
        "            x = self.input_embedding(x)\n",
        "            x = x.unsqueeze(1)  #\n",
        "            features = self.gtrxl(x)\n",
        "            features = features[:, -1, :]  # Tomar la última observación\n",
        "            return features\n"
      ],
      "metadata": {
        "id": "_Ds9Ik44Nmg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de la estrategia de RL que combina el algoritmo PPO y el feature extractor"
      ],
      "metadata": {
        "id": "-mGEN9LMNqvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GTrXLPolicy(ActorCriticPolicy):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space,\n",
        "        action_space,\n",
        "        lr_schedule,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            observation_space,\n",
        "            action_space,\n",
        "            lr_schedule,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        # Parámetros del transformer: cabezas de atención, dimensionalidad y capas\n",
        "        self.features_extractor_kwargs = kwargs.get(\"features_extractor_kwargs\", {}) s\n",
        "        self.d_model = self.features_extractor_kwargs.get(\"d_model\")\n",
        "        self.nheads = self.features_extractor_kwargs.get(\"nheads\")\n",
        "        self.transformer_layers = self.features_extractor_kwargs.get(\"transformer_layers\")\n",
        "\n",
        "    # Construcción de las redes de PPO\n",
        "    def _build(self, lr_schedule) -> None:\n",
        "\n",
        "        self.features_extractor = self.make_features_extractor()\n",
        "\n",
        "\n",
        "        # Crear el MLP extractor de features\n",
        "        self.mlp_extractor = MlpExtractor(\n",
        "            self.features_dim,  # input dim\n",
        "            net_arch=self.net_arch,\n",
        "            activation_fn=self.activation_fn,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Construir las redes de estrategia y valor\n",
        "        latent_dim_pi = self.mlp_extractor.latent_dim_pi\n",
        "        latent_dim_vf = self.mlp_extractor.latent_dim_vf\n",
        "\n",
        "        # Capa de estrategias\n",
        "        if isinstance(self.action_dist, DiagGaussianDistribution):\n",
        "            self.action_net = nn.Linear(latent_dim_pi, self.action_dist.proba_distribution_net_params)\n",
        "        else:  # Categórico\n",
        "            self.action_net = nn.Linear(latent_dim_pi, self.action_space.n)\n",
        "\n",
        "        # Capa de valores\n",
        "        self.value_net = nn.Linear(latent_dim_vf, 1)\n",
        "\n",
        "        # Inicializar pesos\n",
        "        module_gains = {\n",
        "            self.features_extractor: np.sqrt(2),\n",
        "            self.mlp_extractor: np.sqrt(2),\n",
        "            self.action_net: 0.01,\n",
        "            self.value_net: 1,\n",
        "        }\n",
        "\n",
        "        for module, gain in module_gains.items():\n",
        "            module.apply(partial(self.init_weights, gain=gain))\n",
        "\n",
        "        # Optimizador\n",
        "        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1), **self.optimizer_kwargs)\n",
        "\n",
        "    # Crear feature extractor del transformer\n",
        "    def make_features_extractor(self) -> BaseFeaturesExtractor:\n",
        "\n",
        "        return self.features_extractor_class(self.observation_space, **self.features_extractor_kwargs)\n"
      ],
      "metadata": {
        "id": "P-4vRV0_OIhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función que evalua un modelo entrenado calculando la señal media que alcanza"
      ],
      "metadata": {
        "id": "0vUJnXUmOVLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_signal( model, env, max_steps = 20, episodes=10):\n",
        "  mean_total_signal_rl = []\n",
        "\n",
        "\n",
        "  for episode in range(episodes):\n",
        "    print(f\"Episode {episode}\")\n",
        "    obs,_ = env.reset()\n",
        "    done = False\n",
        "    episode_return = 0\n",
        "    total_signal_rl = []\n",
        "    for i in range(max_steps):\n",
        "      # Predice la acción de manera determinística\n",
        "      action, _ = model.predict(obs, deterministic=True)\n",
        "      action = int(action)  # Convertir la acción a un entero\n",
        "      obs, reward, done, _, info = env.step(action)\n",
        "      episode_return += reward\n",
        "\n",
        "      total_signal_rl.append(info['Total_signal'])\n",
        "\n",
        "    mean_total_signal_rl.append(np.mean(total_signal_rl))\n",
        "    return np.mean(mean_total_signal_rl)\n"
      ],
      "metadata": {
        "id": "pTEEtbfYOaTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Busqueda de hiperparámetros con Optuna"
      ],
      "metadata": {
        "id": "G_BGDfLuPDKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_PPOGTrXL(trial):\n",
        "    # Crear entorno con observación parcial\n",
        "    env = PartialObservationEnv(\n",
        "        n=20,\n",
        "        m=20,\n",
        "        max_steps=50,\n",
        "        height=1,\n",
        "        people_quantity=10,\n",
        "        seed=29\n",
        "    )\n",
        "\n",
        "    # Sugerencias de hiperparámetros\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 0.01)\n",
        "    n_steps = trial.suggest_int('n_steps', 2048, 8192)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512])\n",
        "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
        "    ent_coef = trial.suggest_loguniform('ent_coef', 1e-8, 0.1)\n",
        "    vf_coeff = trial.suggest_uniform('vf_coeff', 0.1, 0.9)\n",
        "    clip_range = trial.suggest_uniform('clip_range', 0.1, 0.9)\n",
        "    gae_lambda = trial.suggest_uniform('gae_lambda', 0.8, 1.0)\n",
        "    n_epochs = trial.suggest_int('n_epochs', 3, 10)\n",
        "\n",
        "    num_layers_pi = trial.suggest_int('num_layers1', 1, 3)\n",
        "    layer_sizes_pi = [trial.suggest_categorical(f'layer_{i}_size1', [32, 64, 128]) for i in range(num_layers_pi)]\n",
        "\n",
        "    num_layers_vf = trial.suggest_int('num_layers2', 1, 3)\n",
        "    layer_sizes_vf = [trial.suggest_categorical(f'layer_{i}_size2', [32, 64, 128]) for i in range(num_layers_vf)]\n",
        "\n",
        "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'silu'])\n",
        "    activation_fn_mapping = {\n",
        "        'tanh': nn.Tanh,\n",
        "        'relu': nn.ReLU,\n",
        "        'silu': nn.SiLU\n",
        "    }\n",
        "    activation_fn_class = activation_fn_mapping[activation_fn]\n",
        "\n",
        "\n",
        "    nheads = trial.suggest_int('nheads', 2, 8)\n",
        "\n",
        "    head_dim = trial.suggest_int('d_model', 64, 256, step=32)\n",
        "    d_model = head_dim * nheads\n",
        "    transformer_layers = trial.suggest_int('transformer_layers', 1, 4)\n",
        "\n",
        "    # Resto del código igual...\n",
        "    policy_kwargs = dict(\n",
        "        features_extractor_class=GTrXLFeatureExtractor,\n",
        "        features_extractor_kwargs=dict(\n",
        "            d_model=d_model,\n",
        "            nheads=nheads,\n",
        "            transformer_layers=transformer_layers\n",
        "        ),\n",
        "        net_arch=[dict(pi=layer_sizes_pi, vf=layer_sizes_vf)],\n",
        "        activation_fn=activation_fn_class\n",
        "    )\n",
        "\n",
        "    model = PPO(\n",
        "        policy=GTrXLPolicy,\n",
        "        env=env,\n",
        "        learning_rate=learning_rate,\n",
        "        n_steps=n_steps,\n",
        "        batch_size=batch_size,\n",
        "        gamma=gamma,\n",
        "        ent_coef=ent_coef,\n",
        "        vf_coef=vf_coeff,\n",
        "        clip_range=clip_range,\n",
        "        gae_lambda=gae_lambda,\n",
        "        n_epochs=n_epochs,\n",
        "        policy_kwargs=policy_kwargs,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=100000)\n",
        "    mean_total_signal_rl = mean_signal(model, env)\n",
        "    return mean_total_signal_rl\n",
        "\n",
        "\n",
        "import optuna\n",
        "\n",
        "\n",
        "\n",
        "storage = optuna.storages.RDBStorage(url=\"sqlite:///ppoTr.db\")\n",
        "\n",
        "\n",
        "studyPPO = optuna.create_study(storage=storage, study_name=\"ppoTr\", direction='maximize',load_if_exists=True)\n",
        "studyPPO.optimize(objective_PPOGTrXL, n_trials=200, n_jobs=1)\n",
        "\n",
        "best_trial = studyPPO.best_trial\n",
        "\n",
        "# Imprime los mejores resultados en la consola\n",
        "print(f\"Best trial number: {best_trial.number}\")\n",
        "print(f\"Best trial value (reward): {best_trial.value}\")\n",
        "print(\"Best trial parameters:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Guarda los mejores resultados en un archivo .txt\n",
        "with open('optuna_resultsPPOTr.txt', 'w') as f:\n",
        "    f.write(f\"Best trial number: {best_trial.number}\\n\")\n",
        "    f.write(f\"Best trial value (reward): {best_trial.value}\\n\")\n",
        "    f.write(\"Best trial parameters:\\n\")\n",
        "    for key, value in best_trial.params.items():\n",
        "        f.write(f\"    {key}: {value}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('studyPPOTr.pkl', 'wb') as f:\n",
        "    pickle.dump(studyPPO, f)"
      ],
      "metadata": {
        "id": "s_z0_N_XPGA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento final de los modelos con los resultados de Optuna\n"
      ],
      "metadata": {
        "id": "TiDzrIRFO1jD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwJkfgYmJchd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "env = PartialObservationEnv(\n",
        "    n=20,\n",
        "    m=20,\n",
        "    max_steps=50,\n",
        "    height=1,\n",
        "    people_quantity=10,\n",
        "    seed=29\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Hiperprámetros óptimos para el primer modelo\n",
        "params = {'learning_rate': 0.0006747153326104986, 'n_steps': 8130, 'batch_size': 64, 'gamma': 0.6954486367633275, 'ent_coef': 2.5572784410000594e-07, 'vf_coeff': 0.7185033334025468, 'clip_range': 0.7523046194604548, 'gae_lambda': 0.8661621416239416, 'n_epochs': 4, 'num_layers1': 2, 'layer_0_size1': 64, 'layer_1_size1': 32, 'num_layers2': 2, 'layer_0_size2': 32, 'layer_1_size2': 32, 'activation_fn': 'relu', 'nheads': 3, 'd_model': 192, 'transformer_layers': 1}\n",
        "\n",
        "# Argumentos del transformer\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=GTrXLFeatureExtractor,\n",
        "    features_extractor_kwargs=dict(\n",
        "        d_model=params['d_model'],\n",
        "        nheads=params['nheads'],\n",
        "        transformer_layers=params['transformer_layers']\n",
        "    ),\n",
        "    net_arch=[dict(pi=[32,128,128], vf=[32,32])],\n",
        "    activation_fn= nn.ReLU\n",
        ")\n",
        "\n",
        "# Crear modelo con los hiperparámetros\n",
        "model = PPO(\n",
        "    policy=GTrXLPolicy,\n",
        "    env=env,\n",
        "    learning_rate=params['learning_rate'],\n",
        "    n_steps=params['n_steps'],\n",
        "    batch_size=params['batch_size'],\n",
        "    gamma=params['gamma'],\n",
        "    ent_coef=params['ent_coef'],\n",
        "    vf_coef=params['vf_coeff'],\n",
        "    clip_range=params['clip_range'],\n",
        "    gae_lambda=params['gae_lambda'],\n",
        "    n_epochs=params['n_epochs'],\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Entrenar\n",
        "reward_callback = RewardCallback()\n",
        "model.learn(total_timesteps=500000, callback = reward_callback)\n",
        "\n",
        "\n",
        "\n",
        "model.save(\"./models/env3/transformer1_model.zip\")\n",
        "print(reward_callback.episode_rewards)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Entrenamiento del modelo 2"
      ],
      "metadata": {
        "id": "WnlAp7UyO3G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparámetros óptimos del modelo 2\n",
        "params = {'learning_rate': 0.0055442406400710445, 'n_steps': 8036, 'batch_size': 512, 'gamma': 0.5634271409978006, 'ent_coef': 3.1092876269438148e-06, 'vf_coeff': 0.28952605301403755, 'clip_range': 0.4846436742967982, 'gae_lambda': 0.994042405682114, 'n_epochs': 3, 'num_layers1': 3, 'layer_0_size1': 32, 'layer_1_size1': 128, 'layer_2_size1': 128, 'num_layers2': 2, 'layer_0_size2': 64, 'layer_1_size2': 32, 'activation_fn': 'tanh', 'nheads': 5, 'd_model': 160, 'transformer_layers': 2}\n",
        "\n",
        "\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=GTrXLFeatureExtractor,\n",
        "    features_extractor_kwargs=dict(\n",
        "        d_model=params['d_model'],\n",
        "        nheads=params['nheads'],\n",
        "        transformer_layers=params['transformer_layers']\n",
        "    ),\n",
        "    net_arch=[dict(pi=[32,128,128], vf=[64,32])],\n",
        "    activation_fn= nn.Tanh\n",
        ")\n",
        "\n",
        "model2 = PPO(\n",
        "    policy=GTrXLPolicy,\n",
        "    env=env,\n",
        "    learning_rate=params['learning_rate'],\n",
        "    n_steps=params['n_steps'],\n",
        "    batch_size=params['batch_size'],\n",
        "    gamma=params['gamma'],\n",
        "    ent_coef=params['ent_coef'],\n",
        "    vf_coef=params['vf_coeff'],\n",
        "    clip_range=params['clip_range'],\n",
        "    gae_lambda=params['gae_lambda'],\n",
        "    n_epochs=params['n_epochs'],\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "reward_callback = RewardCallback()\n",
        "model.learn(total_timesteps=500000, callback = reward_callback)\n",
        "\n"
      ],
      "metadata": {
        "id": "gKX7duOxOvDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}